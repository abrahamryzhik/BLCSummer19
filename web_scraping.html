<!doctype html>
<html>
<head>
<meta charset="utf-8">
<link rel="stylesheet" type="text/css" href="style.css">
<title>BLC how to - Home</title>
</head>

<body>

<div class="menu">
	<div class="row">
		<div class="column">
			<div class="logo">
				<div class="smrow">
					<div class = "smcolumn">
						<img class="logo" src="bill_lane_logo.png" height="60px" alt="Smiley face" align="right">
					</div>
					<div class="smcolumn">
						<h3>Bill Lane Center for<br>the American West</h3>
					</div>
				</div>
			</div>
		</div>
		
		<div class="column">
			<bartext>
	  			<p>&nbsp;</p>
	  			<p><a href="index.html">Home</a> &nbsp &nbsp &nbsp <a href="page_1.html">Tutorials</a> &nbsp &nbsp &nbsp <a href="research.html">Our Research</a> &nbsp &nbsp &nbsp <a href="https://west.stanford.edu/">The Center</a> &nbsp &nbsp </p>
			</bartext>	
		</div>

	</div>	
</div>
	
<div class="container">
	<div class="midbox">
		<h1><font color="#DADADA">Web Scraping the EPA Facility Pages</font></h1>
	  <h2>Background on Our Example</h2>
	  <p>Our goal is to scrape information from the detailed faciltiy reports provided by the EPA. Here is an <a href="https://echo.epa.gov/detailed-facility-report?fid=110002762207">example</a> of such a report that we will reference. For this tutorial, we will focus on scraping the demographic information located at the bottom of this page.</p>

	  <h2>Alternative Approaches and Why We Avoid Them</h2>
	  <p>Our first attempt would be to directly use the Python <code>requests</code> library and use a <code>get</code> call to the link above. We would then examine the HTML elements and scrape the information in all the relevant tables. Unfortunately, this method does not work because this page is loaded by JavaScript. Making a request with this method would get the HTML before the JavaScript loads all of the data we want, so the demographics that we want will not appear. </p>
	  <p>Another approach would be to use Selenium to load the JavaScript and then scrape the HTML once the data we want is loaded. A detailed tutorial for this method can be found <a href="http://stanford.edu/~mgorkove/cgi-bin/rpython_tutorials/Scraping_a_Webpage_Rendered_by_Javascript_Using_Python.php">here</a>. This method works, but is somewhat slow due to the need for a browser to fully load each page for us to scrape it. Our goal is to eventually scrape hundreds of thousands of facility pages, so we want to avoid any such slowdowns. What we will do instead is look at the JavaScript for this page to determine where the data is being loaded from and directly receive the data from there.</p>

	  <h2>Finding Where the Data is Coming From</h2>
	  <p>Our first step is to load the JavaScript console for this page. On Chrome, from the <a href="https://echo.epa.gov/detailed-facility-report?fid=110002762207">facility report page</a> go to View-->Developer-->JavaScript Console. The console should appear on the right as below. <br>
	  	<img src="screenshot1.png" height="220px" alt="Smiley Face" class="center">
	  	<br>
	  If we switch from the Console tab on the top to the Network tab, in the Name column you will see many files with the .js ending. Theoretically we may have to search through all these files to find the one that loads the data we want, but luckily they have descriptive names to help us narrow our search. In our case, the file we want to look at is dfr-main.js (dfr stands for Detailed Facility Report). If we open up this file in a new tab, we can look through it and see how the data is loaded by JavaScript. The first function in this file is called <code>getDFR()</code> and this is exactly where we will find our answers. Here is what the beginning of the function looks like: <br>
	</p>
	  <pre>
	  <code>
function getDFR() {
    // check if multiple dfrs loading
    if (fIDArray.length > 1) {
        // if yes, update loading status
        document.getElementById('dfrLoadingStatusReport').innerHTML = currentLoadingFIDIndex + 1;
        document.getElementById('dfrLoadingStatusReports').innerHTML = fIDArray.length;
    }

    // reset for each fIDValue
    noQuarterlyData = true;

    // get fid value from array
    var fIDValue = fIDArray[currentLoadingFIDIndex];

    // show dfr report
    $('#dfr' + fIDValue).show();
    $('.monthly').hide();

    // build proxy url to query with fid
    var myURL = 'app/proxy/proxy.php?s=dfr&p_id=' + fIDValue + (sys == '' ? '' : '&p_system=' + sys);
    fIDValue = decodeURI(fIDValue).replace(' ', '');

    if (Drupal.settings.environment == 'development') {
        console.log(myURL);
    }

    // use ajax to post request
    $.ajax({
        url: myURL,
        type: 'POST',
        dataType: 'json',
        contentType: 'application/json',
        success: function(response) {
	  </code>
	  </pre>
	  <p>
    	There are two extremely useful pieces of information we get from this code. The first is a new URL of the form "app/proxy/proxy.php?s=dfr&p_id=". Since we can see that the code later makes a request to this URL, we suspect (correctly) that this URL is where the code is loading the data from. The id we need to complete the URL we already know from the URL of the detailed facility report to be 110002762207. Before we visit this page to see what is there, we see from the bottom few lines of code that the data we expect to be in JSON form. When we load the url "https://echo.epa.gov/app/proxy/proxy.php?s=dfr&p_id=110002762207" we see the following: <br>
    	<img src="screenshot2.png" height="300px" align="middle">
    	<br>
    	This page contains a JSON file with everything we need! All that's left is getting the data from this page and figuring out how to run this for all the facilities.
	  </p>
	  <h2>Getting the Data for a Facility</h2>
	  <p>
	  	For each facility we will use the <code>requests</code> library to get the information from the URL with the JSON. We then use the JSON object to print out all of the demographic information we want:
	  </p>
	  	<pre>
	  		<code>
import requests
url = "https://echo.epa.gov/app/proxy/proxy.php?s=dfr&p_id=110002762207"
r = requests.get(url)
for data in r.json()['Results']['Demographics']:
    print(data + ": " + r.json()['Results']['Demographics'][data])
	  		</code>
	  	</pre>
	  <p>
	  	After running the code, we should see the following demographic information printed out:
	  </p>
	  <pre>
	  	<code>
Radius: 3
CenterLatitude: 36.68793
CenterLongitude: -119.94582
TotalPersons: 459
LandArea: 100%
WaterArea: 0%
PopulationDensity: 16/sq.mi.
PercentMinority: 42%
Households: 154
HousingUnits: 167
HouseholdsPublicAssistance: 6
PersonsBelowPovertyLevel: 215
White: 327 (71%)
AfricanAmerican: 0 (0%)
HispanicOrigin: 182 (40%)
AsianPacificIslander: 6 (1%)
AmericanIndian: 17 (4%)
OtherMultiracial: 109 (24%)
Child: 28 (6%)
Minors: 122 (27%)
Adults: 337 (73%)
Seniors: 59 (13%)
Less9thGrade: 77 (29.96%)
Grades9to12: 37 (14.4%)
HSDiploma: 68 (26.46%)
SomeCollege: 54 (21.01%)
BSBA: 21 (8.17%)
IncomeLess15k: 10 (9.8%)
Income15to25k: 17 (16.67%)
Income25to50k: 36 (35.29%)
Income50to75k: 13 (12.75%)
Income75kPlus: 26 (25.49%)
	  	</code>
	  </pre>
	  <h2>Speeding it Up</h2>
	  <p>
	  	While the previous section described a very simple way to get all of the information we want from a facility, you may notice when running your code that each request will take a couple of seconds. This is fine if we only want to look at a few facilities, but looping over hundreds of thousands of facilities (which is our goal if we want to look at national trends) will take too long with this method. In this section we describe a way to use threading to significantly speed up the process and make data collection fast. We will be using the <code>ThreadPoolExecutor</code> from <code>concurrent.futures</code> so you will need to <code>import concurrent.futures</code> at the top of your code. Running the <code>ThreadPoolExecutor</code> can be done as follows:
	  </p>
	  <pre>
	  	<code>
with concurrent.futures.ThreadPoolExecutor(max_workers=None) as executor:
        future_to_url = (executor.submit(load_url, url) for url in urls)
        for future in concurrent.futures.as_completed(future_to_url):
            try:
                data = future.result()
            except Exception as exc:
                data = None
            finally:
                print(data)
	  	</code>
	  </pre>
	  <p>
	  	In the code, <code>load_url</code> should be a function you define earlier to process each URL. It will probably look something similar to the code in the previous section of tutorial, but you will want to return some result instead of just printing it. Also in the code, <code>urls</code> should be a list of all the URLs you want to scrape. This list should probably be read from a file and not hard-coded anywhere. Whatever you return from <code>load_url</code> will be placed into the <code>data</code> field when the code does <code>data = future.result()</code>, so while this code simply prints it out you should do whatever is appropriate for your case. Lastly, in this code we have <code>max_workers=None</code> but this can be customized. For more information about the <code>ThreadPoolExecutor</code> we recommend checking out the <a href="https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor">documentation</a>.
	  </p>
	</div>
	<div class=bottombar>
		<center><font color=#6b6b6b><br><br>copyright 2019</font></center>
	</div>
</div>
	

	
</body>
</html>
